{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üìÑ Clasificador de Documentos - Entrenamiento\n",
    "\n",
    "Pipeline completo de extracci√≥n de texto con preprocesamiento autom√°tico para documentos problem√°ticos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import PyPDF2\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWN_PROBLEMATIC_FILES = {\n",
    "    '5bccdc90-25cb-4304-9992-d279443adb0c.pdf',\n",
    "    'b722da60-8b55-4545-b13a-07a4cad1cfbb.pdf', \n",
    "    '435175-128049-file0001.pdf',\n",
    "    '125972-128068-file0001.pdf',\n",
    "    '187429-128122-file0001.pdf',\n",
    "    '436224-128050-file0001.pdf',\n",
    "    '402914-128078-file0001.pdf',\n",
    "    '152400-127701-file0001.pdf'\n",
    "}\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    filename = os.path.basename(pdf_path)\n",
    "    \n",
    "    if filename in KNOWN_PROBLEMATIC_FILES:\n",
    "        # Preprocesamiento seguro\n",
    "        images = convert_from_path(pdf_path, dpi=100)\n",
    "        all_text = []\n",
    "        for image in images:\n",
    "            width, height = image.size\n",
    "            if width * height > 50_000_000:\n",
    "                scale = (50_000_000 / (width * height)) ** 0.5\n",
    "                image = image.resize((int(width * scale), int(height * scale)), Image.Resampling.LANCZOS)\n",
    "            text = pytesseract.image_to_string(image, lang='spa', config='--psm 3')\n",
    "            if text.strip():\n",
    "                all_text.append(text.strip())\n",
    "        return \"\\n\".join(all_text)\n",
    "    else:\n",
    "        # Extracci√≥n est√°ndar\n",
    "        text = \"\"\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text.strip():\n",
    "                    text += page_text + \"\\n\"\n",
    "        \n",
    "        if text.strip():\n",
    "            return text.strip()\n",
    "        else:\n",
    "            # OCR si no hay texto\n",
    "            images = convert_from_path(pdf_path, dpi=300)\n",
    "            ocr_text = []\n",
    "            for image in images:\n",
    "                page_text = pytesseract.image_to_string(image, lang='spa', config='--psm 3')\n",
    "                if page_text.strip():\n",
    "                    ocr_text.append(page_text.strip())\n",
    "            return \"\\n\".join(ocr_text)\n",
    "\n",
    "def extract_text_from_image(filepath):\n",
    "    img = Image.open(filepath)\n",
    "    return pytesseract.image_to_string(img, lang='spa', config='--psm 3').strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_types_and_paths = {\n",
    "    'Carta de trabajo': '../data/Insumos/Carta de trabajo/',\n",
    "    'Cedulas 2': '../data/Insumos/Cedulas 2/',\n",
    "    'Pasaportes': '../data/Insumos/Pasaportes/',\n",
    "    'Sustento de ingresos': '../data/Insumos/Sustento de ingresos/',\n",
    "    'W-9': '../data/Insumos/W-9/'\n",
    "}\n",
    "\n",
    "def extract_corpus_from_documents(doc_paths):\n",
    "    corpus = []\n",
    "    total_processed = 0\n",
    "    total_skipped = 0\n",
    "\n",
    "    for doc_type, base_folder_path in doc_paths.items():\n",
    "        if not os.path.exists(base_folder_path):\n",
    "            continue\n",
    "\n",
    "        for root, dirs, files in os.walk(base_folder_path):\n",
    "            for filename in files:\n",
    "                filepath = os.path.join(root, filename)\n",
    "                file_extension = os.path.splitext(filename)[1].lower()\n",
    "                \n",
    "                try:\n",
    "                    if file_extension == '.pdf':\n",
    "                        extracted_text = extract_text_from_pdf(filepath)\n",
    "                    elif file_extension in ['.png', '.jpg', '.jpeg', '.gif', '.tiff', '.bmp']:\n",
    "                        extracted_text = extract_text_from_image(filepath)\n",
    "                    else:\n",
    "                        total_skipped += 1\n",
    "                        continue\n",
    "\n",
    "                    if extracted_text.strip():\n",
    "                        corpus.append((extracted_text, doc_type))\n",
    "                        total_processed += 1\n",
    "                    else:\n",
    "                        total_skipped += 1\n",
    "                except:\n",
    "                    total_skipped += 1\n",
    "    \n",
    "    return corpus, total_processed, total_skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documentos: 344\n",
      "Tipos: 5\n",
      "\n",
      "Distribuci√≥n:\n",
      "  Carta de trabajo: 98\n",
      "  Sustento de ingresos: 95\n",
      "  Pasaportes: 91\n",
      "  Cedulas 2: 41\n",
      "  W-9: 19\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documento_original</th>\n",
       "      <th>tipo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>o!\\n\\n(O) Tintorer√≠a Ecoloca\\nN TACO\\n\\nPara: ...</td>\n",
       "      <td>Carta de trabajo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CICR\\n\\nCONSTANCIA DE TRABAJO\\n\\nA quien pueda...</td>\n",
       "      <td>Carta de trabajo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Distribuidora Zoliannys, LLC.\\n\\nEIN 30-130830...</td>\n",
       "      <td>Carta de trabajo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CONSTANCIA\\n\\nAla atenci√≥n de Banco Mercantil ...</td>\n",
       "      <td>Carta de trabajo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ave. Samuel Lewis y Calle 58, Obarrio , Torre ...</td>\n",
       "      <td>Carta de trabajo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  documento_original              tipo\n",
       "0  o!\\n\\n(O) Tintorer√≠a Ecoloca\\nN TACO\\n\\nPara: ...  Carta de trabajo\n",
       "1  CICR\\n\\nCONSTANCIA DE TRABAJO\\n\\nA quien pueda...  Carta de trabajo\n",
       "2  Distribuidora Zoliannys, LLC.\\n\\nEIN 30-130830...  Carta de trabajo\n",
       "3  CONSTANCIA\\n\\nAla atenci√≥n de Banco Mercantil ...  Carta de trabajo\n",
       "4  Ave. Samuel Lewis y Calle 58, Obarrio , Torre ...  Carta de trabajo"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_file_path = '../data/processed_corpus.pkl'\n",
    "\n",
    "if os.path.exists(corpus_file_path):\n",
    "    df_corpus = joblib.load(corpus_file_path)\n",
    "else:\n",
    "    extracted_corpus_list, total_processed, total_skipped = extract_corpus_from_documents(document_types_and_paths)\n",
    "    df_corpus = pd.DataFrame(extracted_corpus_list, columns=['documento_original', 'tipo'])\n",
    "    \n",
    "    if not df_corpus.empty:\n",
    "        os.makedirs(os.path.dirname(corpus_file_path), exist_ok=True)\n",
    "        joblib.dump(df_corpus, corpus_file_path)\n",
    "\n",
    "# Estad√≠sticas\n",
    "print(f\"Total documentos: {len(df_corpus)}\")\n",
    "print(f\"Tipos: {df_corpus['tipo'].nunique()}\")\n",
    "print(\"\\nDistribuci√≥n:\")\n",
    "for doc_type, count in df_corpus['tipo'].value_counts().items():\n",
    "    print(f\"  {doc_type}: {count}\")\n",
    "\n",
    "df_corpus.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargado el recurso WordNet de NLTK (para lematizaci√≥n).\n",
      "Configuraci√≥n de NLTK completada.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/mb/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer # O PorterStemmer para stemming\n",
    "\n",
    "# Descargar recursos de NLTK necesarios\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "    print(\"Descargado el recurso WordNet de NLTK (para lematizaci√≥n).\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    print(\"Descargadas las stopwords de NLTK.\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    print(\"Descargado el tokenizador Punkt de NLTK.\")\n",
    "\n",
    "print(\"Configuraci√≥n de NLTK completada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Funci√≥n de preprocesamiento mejorada definida\n",
      "‚úÖ Incluye detecci√≥n de idioma y preservaci√≥n selectiva de n√∫meros\n"
     ]
    }
   ],
   "source": [
    "# Funci√≥n de preprocesamiento mejorada\n",
    "from langdetect import detect\n",
    "import spacy\n",
    "\n",
    "# Funci√≥n para detectar idioma\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'es'  # Default a espa√±ol\n",
    "\n",
    "# Funci√≥n mejorada de preprocesamiento\n",
    "def preprocess_text_improved(text, preserve_numbers=True):\n",
    "    \"\"\"\n",
    "    Preprocesamiento mejorado que:\n",
    "    - Detecta el idioma autom√°ticamente\n",
    "    - Usa stopwords apropiadas seg√∫n el idioma\n",
    "    - Preserva n√∫meros importantes (opcional)\n",
    "    - Maneja documentos muy cortos\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or len(text.strip()) < 10:\n",
    "        return text if isinstance(text, str) else \"\"\n",
    "    \n",
    "    # Detectar idioma\n",
    "    lang = detect_language(text)\n",
    "    \n",
    "    # Configurar stopwords seg√∫n idioma detectado\n",
    "    if lang == 'en':\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    else:  # Default espa√±ol\n",
    "        stop_words = set(stopwords.words('spanish'))\n",
    "    \n",
    "    # Convertir a min√∫sculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Eliminar puntuaci√≥n pero preservar algunos caracteres importantes\n",
    "    text = text.translate(str.maketrans('', '', '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'))\n",
    "    \n",
    "    # Preservar o eliminar n√∫meros seg√∫n par√°metro\n",
    "    if not preserve_numbers:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "    else:\n",
    "        # Preservar solo n√∫meros que parecen importantes (fechas, c√≥digos, etc.)\n",
    "        # Mantener n√∫meros de 4+ d√≠gitos (a√±os, c√≥digos) y n√∫meros con puntos/guiones\n",
    "        text = re.sub(r'\\b\\d{1,3}\\b(?!\\d)', '', text)  # Eliminar n√∫meros de 1-3 d√≠gitos solos\n",
    "    \n",
    "    # Tokenizaci√≥n\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Eliminar stop words y palabras muy cortas\n",
    "    processed_tokens = []\n",
    "    for word in tokens:\n",
    "        if (len(word) > 2 and \n",
    "            word not in stop_words and \n",
    "            not word.isdigit() or len(word) > 3):  # Preservar n√∫meros largos\n",
    "            processed_tokens.append(word)\n",
    "    \n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "print(\"üîß Funci√≥n de preprocesamiento mejorada definida\")\n",
    "print(\"‚úÖ Incluye detecci√≥n de idioma y preservaci√≥n selectiva de n√∫meros\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Funci√≥n de preprocesamiento optimizada para clasificaci√≥n definida\n",
      "‚úÖ Incluye normalizaci√≥n de acentos y preservaci√≥n de palabras clave del dominio\n"
     ]
    }
   ],
   "source": [
    "# üîß FUNCI√ìN DE PREPROCESAMIENTO MEJORADA PARA CLASIFICACI√ìN\n",
    "import unicodedata\n",
    "\n",
    "def preprocess_text_for_classification(text, preserve_domain_keywords=True):\n",
    "    \"\"\"\n",
    "    Preprocesamiento optimizado para clasificaci√≥n de documentos:\n",
    "    - Normaliza acentos y caracteres especiales\n",
    "    - Preserva palabras clave del dominio\n",
    "    - Limpia ruido del OCR de manera inteligente\n",
    "    - Maneja n√∫meros de forma contextual\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or len(text.strip()) < 5:\n",
    "        return text if isinstance(text, str) else \"\"\n",
    "    \n",
    "    # Detectar idioma\n",
    "    lang = detect_language(text)\n",
    "    \n",
    "    # Configurar stopwords seg√∫n idioma\n",
    "    if lang == 'en':\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    else:\n",
    "        stop_words = set(stopwords.words('spanish'))\n",
    "    \n",
    "    # Palabras clave importantes del dominio (no eliminar)\n",
    "    domain_keywords = {\n",
    "        'pasaporte', 'passport', 'cedula', 'c√©dula', 'carta', 'trabajo', 'constancia',\n",
    "        'form', 'w9', 'taxpayer', 'sustento', 'ingresos', 'informe', 'contador',\n",
    "        'republica', 'venezuela', 'bolivariana', 'director', 'ein', 'social', 'security'\n",
    "    }\n",
    "    \n",
    "    # 1. Convertir a min√∫sculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Normalizar acentos y caracteres especiales\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')\n",
    "    \n",
    "    # 3. Limpiar ruido com√∫n del OCR\n",
    "    text = re.sub(r'[|\\\\/*+{}[\\]()<>]', ' ', text)  # Caracteres extra√±os del OCR\n",
    "    text = re.sub(r'[^\\w\\s.-]', ' ', text)  # Mantener solo letras, n√∫meros, espacios, puntos y guiones\n",
    "    text = re.sub(r'\\s+', ' ', text)  # M√∫ltiples espacios a uno solo\n",
    "    \n",
    "    # 4. Tokenizaci√≥n\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # 5. Procesamiento inteligente de tokens\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        # Saltar tokens muy cortos (menos de 2 caracteres)\n",
    "        if len(token) < 2:\n",
    "            continue\n",
    "            \n",
    "        # Preservar palabras clave del dominio\n",
    "        if token in domain_keywords:\n",
    "            processed_tokens.append(token)\n",
    "            continue\n",
    "            \n",
    "        # Preservar n√∫meros importantes (4+ d√≠gitos, fechas, c√≥digos)\n",
    "        if re.match(r'^\\d{4,}$|^\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}$|^v\\d+$', token):\n",
    "            processed_tokens.append(token)\n",
    "            continue\n",
    "            \n",
    "        # Eliminar stopwords pero no palabras del dominio\n",
    "        if token not in stop_words and len(token) > 2:\n",
    "            processed_tokens.append(token)\n",
    "    \n",
    "    # 6. Limitar longitud final si es muy larga\n",
    "    final_text = ' '.join(processed_tokens[:100])  # M√°ximo 100 tokens\n",
    "    \n",
    "    return final_text if final_text else text[:50]  # Fallback para textos muy problem√°ticos\n",
    "\n",
    "print(\"üîß Funci√≥n de preprocesamiento optimizada para clasificaci√≥n definida\")\n",
    "print(\"‚úÖ Incluye normalizaci√≥n de acentos y preservaci√≥n de palabras clave del dominio\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar el preprocesamiento al corpus\n",
    "df_corpus['texto_procesado'] = df_corpus['documento_original'].apply(preprocess_text_for_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Estad√≠sticas del corpus procesado:\n",
      "\n",
      "üìè Longitud promedio de documentos: 602.15 caracteres\n",
      "\n",
      "üìë Distribuci√≥n de tipos de documentos:\n",
      "tipo\n",
      "Carta de trabajo        98\n",
      "Sustento de ingresos    95\n",
      "Pasaportes              91\n",
      "Cedulas 2               41\n",
      "W-9                     19\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üî§ Palabras √∫nicas por tipo de documento:\n",
      "\n",
      "Carta de trabajo: 2977 palabras √∫nicas\n",
      "\n",
      "Cedulas 2: 381 palabras √∫nicas\n",
      "\n",
      "Pasaportes: 2001 palabras √∫nicas\n",
      "\n",
      "Sustento de ingresos: 2339 palabras √∫nicas\n",
      "\n",
      "W-9: 437 palabras √∫nicas\n",
      "\n",
      "üíæ Corpus preprocesado guardado en: ../data/preprocessed_corpus.pkl\n",
      "\n",
      "üîç Cargado desde archivo pkl:\n",
      "                                  documento_original              tipo  \\\n",
      "0  o!\\n\\n(O) Tintorer√≠a Ecoloca\\nN TACO\\n\\nPara: ...  Carta de trabajo   \n",
      "1  CICR\\n\\nCONSTANCIA DE TRABAJO\\n\\nA quien pueda...  Carta de trabajo   \n",
      "2  Distribuidora Zoliannys, LLC.\\n\\nEIN 30-130830...  Carta de trabajo   \n",
      "3  CONSTANCIA\\n\\nAla atenci√≥n de Banco Mercantil ...  Carta de trabajo   \n",
      "4  Ave. Samuel Lewis y Calle 58, Obarrio , Torre ...  Carta de trabajo   \n",
      "\n",
      "                                     texto_procesado  \n",
      "0  tintoreria ecoloca taco mercantil banco consta...  \n",
      "1  cicr constancia trabajo pueda interesar. suscr...  \n",
      "2  distribuidora zoliannys llc. ein 30-1308305 fl...  \n",
      "3  constancia ala atencion banco mercantil panama...  \n",
      "4  ave. samuel lewis calle obarrio torre adr piso...  \n",
      "\n",
      "üíæ Corpus preprocesado guardado en CSV: ../data/preprocessed_corpus.csv\n",
      "\n",
      "üîç Cargado desde archivo CSV:\n",
      "                                  documento_original              tipo  \\\n",
      "0  o!\\n\\n(O) Tintorer√≠a Ecoloca\\nN TACO\\n\\nPara: ...  Carta de trabajo   \n",
      "1  CICR\\n\\nCONSTANCIA DE TRABAJO\\n\\nA quien pueda...  Carta de trabajo   \n",
      "2  Distribuidora Zoliannys, LLC.\\n\\nEIN 30-130830...  Carta de trabajo   \n",
      "3  CONSTANCIA\\n\\nAla atenci√≥n de Banco Mercantil ...  Carta de trabajo   \n",
      "4  Ave. Samuel Lewis y Calle 58, Obarrio , Torre ...  Carta de trabajo   \n",
      "\n",
      "                                     texto_procesado  \n",
      "0  tintoreria ecoloca taco mercantil banco consta...  \n",
      "1  cicr constancia trabajo pueda interesar. suscr...  \n",
      "2  distribuidora zoliannys llc. ein 30-1308305 fl...  \n",
      "3  constancia ala atencion banco mercantil panama...  \n",
      "4  ave. samuel lewis calle obarrio torre adr piso...  \n"
     ]
    }
   ],
   "source": [
    "# Generar estad√≠sticas del corpus procesado\n",
    "print(\"üìä Estad√≠sticas del corpus procesado:\\n\")\n",
    "\n",
    "# Longitud promedio de documentos\n",
    "longitud_promedio = df_corpus['texto_procesado'].str.len().mean()\n",
    "print(f\"üìè Longitud promedio de documentos: {longitud_promedio:.2f} caracteres\")\n",
    "\n",
    "# Distribuci√≥n de tipos de documentos\n",
    "distribucion_tipos = df_corpus['tipo'].value_counts()\n",
    "print(\"\\nüìë Distribuci√≥n de tipos de documentos:\")\n",
    "print(distribucion_tipos)\n",
    "\n",
    "# Palabras √∫nicas por tipo\n",
    "print(\"\\nüî§ Palabras √∫nicas por tipo de documento:\")\n",
    "for tipo in df_corpus['tipo'].unique():\n",
    "    palabras_tipo = set(' '.join(df_corpus[df_corpus['tipo'] == tipo]['texto_procesado']).split())\n",
    "    print(f\"\\n{tipo}: {len(palabras_tipo)} palabras √∫nicas\")\n",
    "\n",
    "# Guardar el corpus preprocesado en archivo pkl\n",
    "ruta_guardado = '../data/preprocessed_corpus.pkl'\n",
    "df_corpus.to_pickle(ruta_guardado)\n",
    "print(f\"\\nüíæ Corpus preprocesado guardado en: {ruta_guardado}\")\n",
    "\n",
    "# Cargar el corpus preprocesado desde archivo pkl\n",
    "df_corpus_cargado = pd.read_pickle(ruta_guardado)\n",
    "print(\"\\nüîç Cargado desde archivo pkl:\")\n",
    "print(df_corpus_cargado.head())\n",
    "\n",
    "# Guardar tambi√©n en formato CSV\n",
    "ruta_csv = '../data/preprocessed_corpus.csv'\n",
    "df_corpus.to_csv(ruta_csv, index=False)\n",
    "print(f\"\\nüíæ Corpus preprocesado guardado en CSV: {ruta_csv}\")\n",
    "\n",
    "# Cargar el corpus desde CSV para verificar\n",
    "df_corpus_csv = pd.read_csv(ruta_csv)\n",
    "print(\"\\nüîç Cargado desde archivo CSV:\")\n",
    "print(df_corpus_csv.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen2-vl-finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
